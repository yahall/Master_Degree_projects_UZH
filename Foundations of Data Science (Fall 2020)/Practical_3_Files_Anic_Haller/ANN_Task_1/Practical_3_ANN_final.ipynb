{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practical_3_ANN_Yannik_V1_6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dastuzh/M49-FDS20/blob/master/Practical_3_Files_Anic_Haller/ANN_Task_1/Practical_3_ANN_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t464G-aBHbB"
      },
      "source": [
        "# Practical 3: Artificial Neural Network\n",
        "\n",
        "This is the first task of Practical 3. You will build a neural network to classify the handwritten digits from the MNIST dataset (http://yann.lecun.com/exdb/mnist/). \n",
        "You will build the classifier from scartch. \n",
        "\n",
        "We will mark your code based on the accuracy of your model. You should get **at least 97%** accuracy on this dataset. Don't forget to save and check in your model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTXs8t50BHbF"
      },
      "source": [
        "## Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knBNP7RRBHbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e86b890c-e253-4d10-a5a2-486929e0d198"
      },
      "source": [
        "# We load the TensorBoard notebook extension and import tensorboard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Import the standard packages for this practical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# We import some tools of Keras directly, such that our code gets more concise1\n",
        "from keras.layers import Dense, Activation, Flatten, BatchNormalization, ELU, Dropout\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Import the datatime and os package, which we need to label our training histories nicely in tensorboard\n",
        "import datetime, os\n",
        "\n",
        "# Import the package CyclicLR to use cyclical learning rates\n",
        "!git clone https://github.com/bckenstler/CLR.git\n",
        "from CLR.clr_callback import CyclicLR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CLR'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 252 (delta 1), reused 4 (delta 0), pack-reused 244\u001b[K\n",
            "Receiving objects: 100% (252/252), 2.06 MiB | 26.35 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZuBG5fIMBHbG",
        "outputId": "96cf99ac-bd1c-4e67-aa31-4d1feeda17dd"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AMLuMgdCBHbH",
        "outputId": "3811cad3-2ece-4562-9350-8d972d1e9127"
      },
      "source": [
        "tf.keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXeAxbthNZOh"
      },
      "source": [
        "# Set global parameters for figures\r\n",
        "%matplotlib inline\r\n",
        "plt.style.use('seaborn-white')\r\n",
        "plt.rcParams['lines.linewidth'] = 3\r\n",
        "plt.rcParams['figure.figsize'] = (10,6)\r\n",
        "plt.rcParams['figure.titlesize'] = 20\r\n",
        "plt.rcParams['axes.titlesize'] = 18\r\n",
        "plt.rcParams['axes.labelsize'] = 14\r\n",
        "plt.rcParams['legend.fontsize'] = 14"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O00jZBuGBHbI"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "In this block, you will prepare the data for the training, such as normalisation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0T0GRbrYBHbI",
        "outputId": "9b68cf55-50c4-468c-ab94-1ad838494807"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "for i in range(0,9):\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    plt.imshow(X_train_full[i])\n",
        "plt.show()\n",
        "\n",
        "X_train_full.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yu1hKRZFTZ8"
      },
      "source": [
        "# Normalize the input features, since modifying the values of each pixel such that they range from 0 to 1 will improve the rate at which our model learns \r\n",
        "## Note: we devide by a floating number, such that the integers are transformed into floats\r\n",
        "X_train_full = X_train_full/255.0\r\n",
        "X_test = X_test/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O96bStdGCEV",
        "outputId": "ef544929-f93f-406e-8128-fb0ae2cf654c"
      },
      "source": [
        "# Transform the classes within the label into a one-hot encoding matrix\r\n",
        "y_train_full = keras.utils.to_categorical(y_train_full, num_classes=10)\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=10)\r\n",
        "\r\n",
        "y_train_full[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjQaV7loHIk4"
      },
      "source": [
        "# Get a validation dataset. We decide to take 6000 randomly chosen images from the full training data as the validation dataset\r\n",
        "np.random.seed(77)\r\n",
        "shuffler = np.random.permutation(X_train_full.shape[0])\r\n",
        "X_valid, X_train = X_train_full[shuffler[:6000]], X_train_full[shuffler[6000:]]\r\n",
        "y_valid, y_train = y_train_full[shuffler[:6000]], y_train_full[shuffler[6000:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0odsW7N8BHbI"
      },
      "source": [
        "## Build and train the model\n",
        "\n",
        "Build and train your model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK_97_tk_ymc"
      },
      "source": [
        "# TODO: Build your model\r\n",
        "\r\n",
        "## Initial model:\r\n",
        "def create_init_model():\r\n",
        "\r\n",
        "  # First we set up a Sequential Keras Model\r\n",
        "  model = Sequential()\r\n",
        "  # Add a flatten layer to transfrom the 28x28 inputs into a 1D array of length 784 (=28x28)\r\n",
        "  model.add(Flatten(input_shape=[28, 28]))\r\n",
        "  # Add a sequence of dense hidden layers with a decreasing number of neurons and relu activation functions\r\n",
        "  # Note: These layers include a vector of bias terms by default\r\n",
        "  model.add(Dense(500, activation=\"relu\", kernel_initializer=\"he_normal\"))\r\n",
        "  model.add(Dense(200, activation=\"relu\", kernel_initializer=\"he_normal\"))\r\n",
        "  model.add(Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\r\n",
        "  model.add(Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\"))\r\n",
        "  # Add a dense output layer with 10 neurons (i.e. one per class in the label)\r\n",
        "  model.add(Dense(10, activation=\"softmax\"))\r\n",
        "\r\n",
        "  # Compile the model (Normal)\r\n",
        "  ## Note:\r\n",
        "  ### As usual in classification problems we use the categorical crossentropy as the loss function\r\n",
        "  ### We use the Adam method as the optimizer with learning rate 0.001\r\n",
        "  ### We set the argument metrics to \"accuracy\", such that Keras computes and outputs the accuracy of the model on the validation set while it is trained\r\n",
        "  model.compile(loss=\"categorical_crossentropy\",\r\n",
        "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\r\n",
        "                metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "  # Return the model\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ynK1PFfMK88"
      },
      "source": [
        "# Define Callbacks\r\n",
        "## To ensure that our model training histories are labelled nicely in tensorboard including a time indication of when the model has been fitted, \r\n",
        "## we define a variable to capture the exact logdirectory and assign this directory to the tensorboard callback afterwards.\r\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n",
        "tensorboard_cb = TensorBoard(logdir, histogram_freq=1)\r\n",
        "\r\n",
        "# To prevent overfitting, we introduce early stopping, such that the training is stopped as soon as no improovement has been monitored over 7 periods according to the validation accuracy\r\n",
        "early_stopping_cb = EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huMdahHdAP8L",
        "outputId": "c65ca2f8-9368-44fd-8e44-f07f3e9bda29"
      },
      "source": [
        "# Train the initial model for different batch sizes\r\n",
        "for bs in np.arange(30,110,10):\r\n",
        "  # Create the initial model\r\n",
        "  model = create_init_model()\r\n",
        "  # Train the model\r\n",
        "  history = model.fit(X_train, y_train, batch_size=bs, epochs=40, verbose=0,\r\n",
        "                      validation_data=(X_valid, y_valid), \r\n",
        "                      callbacks=[early_stopping_cb, tensorboard_cb])\r\n",
        "  # Calculate and display its validation accuracy for differing batch sizes\r\n",
        "  val_acc = model.evaluate(X_valid, y_valid, verbose=0)[1]\r\n",
        "  print('Using a batch size of', bs, 'the initial model achieves a validation accuracy of', val_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using a batch size of 30 the initial model achieves a validation accuracy of 0.981166660785675\n",
            "Using a batch size of 40 the initial model achieves a validation accuracy of 0.9816666841506958\n",
            "Using a batch size of 50 the initial model achieves a validation accuracy of 0.9831666946411133\n",
            "Using a batch size of 60 the initial model achieves a validation accuracy of 0.981333315372467\n",
            "Using a batch size of 70 the initial model achieves a validation accuracy of 0.9825000166893005\n",
            "Using a batch size of 80 the initial model achieves a validation accuracy of 0.9815000295639038\n",
            "Using a batch size of 90 the initial model achieves a validation accuracy of 0.9825000166893005\n",
            "Using a batch size of 100 the initial model achieves a validation accuracy of 0.981333315372467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7hg_olycUOA"
      },
      "source": [
        "# Define a function to create and compile a model\r\n",
        "def create_model(n_hidden=2, nh_neurons=32, activ=\"relu\", lr=0.001, batch_norm = False):\r\n",
        "  # Depending on the activation function, set up the accordingly 'best' kernel initialization, which is \"he_normal\" for relu-type\r\n",
        "  # activation functions and \"lecun_normal\" for the selu activation function. Since we only try-out relu, elu and selu, these two\r\n",
        "  # initialization types should be sufficient.\r\n",
        "  if activ == \"selu\":\r\n",
        "    k_init = \"lecun_normal\"\r\n",
        "  else:\r\n",
        "    k_init = \"he_normal\"\r\n",
        "  # Set up a sequential model\r\n",
        "  model = Sequential()\r\n",
        "  # Flatten layer\r\n",
        "  model.add(Flatten(input_shape=[28, 28]))\r\n",
        "\r\n",
        "  if batch_norm:\r\n",
        "    # n_hidden Low level hidden layers\r\n",
        "    for layer in range(n_hidden):\r\n",
        "      nl_neurons = np.min([(4*nh_neurons),280])*2\r\n",
        "      if nl_neurons < 300:\r\n",
        "        nl_neurons = 400\r\n",
        "      if nl_neurons > 560:\r\n",
        "        nl_neurons = 500\r\n",
        "      model.add(Dense(nl_neurons, activation=activ, kernel_initializer=k_init))\r\n",
        "      model.add(BatchNormalization())\r\n",
        "\r\n",
        "    # 3 High level hidden layers\r\n",
        "    model.add(Dense(np.min([(4*nh_neurons),280]), activation=activ, kernel_initializer=k_init))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dense((2*nh_neurons), activation=activ, kernel_initializer=k_init))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dense(nh_neurons, activation=activ, kernel_initializer=k_init))\r\n",
        "\r\n",
        "  else:\r\n",
        "    # Flatten layer\r\n",
        "    model.add(Flatten(input_shape=[28, 28]))\r\n",
        "  \r\n",
        "    # n_hidden Low level hidden layers\r\n",
        "    for layer in range(n_hidden):\r\n",
        "      nl_neurons = np.min([(4*nh_neurons),280])*2\r\n",
        "      if nl_neurons < 300:\r\n",
        "        nl_neurons = 400\r\n",
        "      if nl_neurons > 560:\r\n",
        "        nl_neurons = 500\r\n",
        "      model.add(Dense(nl_neurons, activation=activ, kernel_initializer=k_init))\r\n",
        "  \r\n",
        "    # 3 High level hidden layers\r\n",
        "    model.add(Dense(np.min([(4*nh_neurons),280]), activation=activ, kernel_initializer=k_init))\r\n",
        "    model.add(Dense((2*nh_neurons), activation=activ, kernel_initializer=k_init))\r\n",
        "    model.add(Dense(nh_neurons, activation=activ, kernel_initializer=k_init))\r\n",
        "\r\n",
        "  # Add a dense output layer with 10 neurons\r\n",
        "  model.add(Dense(10, activation=\"softmax\"))\r\n",
        "\r\n",
        "  # Compile the model\r\n",
        "  model.compile(loss=\"categorical_crossentropy\",\r\n",
        "              optimizer=Adam(learning_rate=lr),\r\n",
        "              metrics=[\"accuracy\"])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwXYI0THwZH2"
      },
      "source": [
        "# Create a function to perform grid search\r\n",
        "def best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[0,2], nh_neurons_range=[30,50,70,100], \r\n",
        "             activations=[\"relu\",\"elu\"], lr_range=[0.001,0.0005], batch_norm_range=[True,False]):\r\n",
        "  # Create a variable to store the best score seen so far\r\n",
        "  best_score = 0\r\n",
        "  # To prevent overfitting, we introduce early stopping, such that the training is stopped as soon as no improovement has been monitored over 7 periods according to the validation accuracy\r\n",
        "  early_stopping_cb = EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)\r\n",
        "  # Set up a loop to go through all possible parameter combinations\r\n",
        "  for n_hidden in range(n_hidden_range[0],(n_hidden_range[1]+1)):\r\n",
        "    for nh_neurons in nh_neurons_range:\r\n",
        "      for activ in activations:\r\n",
        "        for lr in lr_range:\r\n",
        "          for batch_norm in batch_norm_range:\r\n",
        "            model = create_model(n_hidden, nh_neurons, activ, lr, batch_norm)\r\n",
        "            # Train the model\r\n",
        "            model.fit(X_train, y_train, batch_size=50, epochs=40, verbose=0,\r\n",
        "                      validation_data=(X_valid, y_valid), \r\n",
        "                      callbacks=[early_stopping_cb])\r\n",
        "            loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "            print(\"The parameter combination\", [n_hidden, nh_neurons, activ, lr, batch_norm], \"leads to a validation accuracy of\", model.evaluate(X_valid, y_valid, verbose=0)[1])\r\n",
        "            if accuracy > best_score:\r\n",
        "              best_score = accuracy\r\n",
        "              best_model = model\r\n",
        "              best_params = [n_hidden, nh_neurons, activ, lr, batch_norm]\r\n",
        "              print(\"The best test accuracy so far is:\", best_score)\r\n",
        "\r\n",
        "  return best_model, best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttpmh7htwewj",
        "outputId": "c5ea483d-7d14-4493-d6e4-4699c316d0ca"
      },
      "source": [
        "# Apply the best_hyp function to find an optimal range for the number of hidden layers\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[0,5], \r\n",
        "                         nh_neurons_range=[50], activations=[\"relu\"], lr_range=[0.001],\r\n",
        "                         batch_norm_range=[False])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameter combination [0, 50, 'relu', 0.001, False] leads to a validation accuracy of 0.981333315372467\n",
            "The best test accuracy so far is: 0.982200026512146\n",
            "The parameter combination [1, 50, 'relu', 0.001, False] leads to a validation accuracy of 0.9819999933242798\n",
            "The parameter combination [2, 50, 'relu', 0.001, False] leads to a validation accuracy of 0.984333336353302\n",
            "The best test accuracy so far is: 0.9829000234603882\n",
            "The parameter combination [3, 50, 'relu', 0.001, False] leads to a validation accuracy of 0.9821666479110718\n",
            "The parameter combination [4, 50, 'relu', 0.001, False] leads to a validation accuracy of 0.9801666736602783\n",
            "The parameter combination [5, 50, 'relu', 0.001, False] leads to a validation accuracy of 0.981333315372467\n",
            "The best parameters are: [2, 50, 'relu', 0.001, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HncBFtq2MhV_",
        "outputId": "f1fe598d-0696-4100-db1c-803553cd61fd"
      },
      "source": [
        "# Apply the best_hyp function to find an optimal range for the number of neurons in the last hidden layer\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[2,2], \r\n",
        "                         nh_neurons_range=np.arange(30,110,10), activations=[\"relu\"], lr_range=[0.001],\r\n",
        "                         batch_norm_range=[False])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameter combination [2, 30, 'relu', 0.001, False] leads to a validation accuracy of 0.9808333516120911\n",
            "The best test accuracy so far is: 0.98089998960495\n",
            "The parameter combination [2, 40, 'relu', 0.001, False] leads to a validation accuracy of 0.9803333282470703\n",
            "The parameter combination [2, 50, 'relu', 0.001, False] leads to a validation accuracy of 0.9788333177566528\n",
            "The best test accuracy so far is: 0.9812999963760376\n",
            "The parameter combination [2, 60, 'relu', 0.001, False] leads to a validation accuracy of 0.9806666374206543\n",
            "The parameter combination [2, 70, 'relu', 0.001, False] leads to a validation accuracy of 0.9816666841506958\n",
            "The parameter combination [2, 80, 'relu', 0.001, False] leads to a validation accuracy of 0.981166660785675\n",
            "The best test accuracy so far is: 0.9818999767303467\n",
            "The parameter combination [2, 90, 'relu', 0.001, False] leads to a validation accuracy of 0.9819999933242798\n",
            "The parameter combination [2, 100, 'relu', 0.001, False] leads to a validation accuracy of 0.9826666712760925\n",
            "The best test accuracy so far is: 0.9830999970436096\n",
            "The best parameters are: [2, 100, 'relu', 0.001, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izx75bVASYsy",
        "outputId": "e9e43251-ed86-46e2-b64b-481345792cfb"
      },
      "source": [
        "# Apply the best_hyp function to find an optimal set of activation functions\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[2,2], \r\n",
        "                         nh_neurons_range=[100], activations=[\"relu\",\"elu\",\"selu\"], lr_range=[0.001],\r\n",
        "                         batch_norm_range=[False])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameter combination [2, 100, 'relu', 0.001, False] leads to a validation accuracy of 0.9815000295639038\n",
            "The best test accuracy so far is: 0.9814000129699707\n",
            "The parameter combination [2, 100, 'elu', 0.001, False] leads to a validation accuracy of 0.981333315372467\n",
            "The parameter combination [2, 100, 'selu', 0.001, False] leads to a validation accuracy of 0.9823333621025085\n",
            "The best parameters are: [2, 100, 'relu', 0.001, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "venRwp_LZfzs",
        "outputId": "0a1df81f-f8cf-407f-84ae-7fd1a5d40ed6"
      },
      "source": [
        "# Apply the best_hyp function to find an optimal range for learning rate\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[2,2], \r\n",
        "                         nh_neurons_range=[100], activations=[\"relu\"], \r\n",
        "                         lr_range=[0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009,0.001],\r\n",
        "                         batch_norm_range=[False])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameter combination [2, 100, 'relu', 0.0001, False] leads to a validation accuracy of 0.9825000166893005\n",
            "The best test accuracy so far is: 0.9828000068664551\n",
            "The parameter combination [2, 100, 'relu', 0.0002, False] leads to a validation accuracy of 0.9819999933242798\n",
            "The parameter combination [2, 100, 'relu', 0.0003, False] leads to a validation accuracy of 0.9831666946411133\n",
            "The parameter combination [2, 100, 'relu', 0.0004, False] leads to a validation accuracy of 0.981333315372467\n",
            "The parameter combination [2, 100, 'relu', 0.0005, False] leads to a validation accuracy of 0.9819999933242798\n",
            "The parameter combination [2, 100, 'relu', 0.0006, False] leads to a validation accuracy of 0.9794999957084656\n",
            "The parameter combination [2, 100, 'relu', 0.0007, False] leads to a validation accuracy of 0.9819999933242798\n",
            "The parameter combination [2, 100, 'relu', 0.0008, False] leads to a validation accuracy of 0.9819999933242798\n",
            "The parameter combination [2, 100, 'relu', 0.0009, False] leads to a validation accuracy of 0.9838333129882812\n",
            "The best test accuracy so far is: 0.9829999804496765\n",
            "The parameter combination [2, 100, 'relu', 0.001, False] leads to a validation accuracy of 0.9838333129882812\n",
            "The best parameters are: [2, 100, 'relu', 0.0009, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3vCeokoo07E"
      },
      "source": [
        "# Try whether the model performs better when using the Cyclical LR callback\r\n",
        "\r\n",
        "# Create the model\r\n",
        "model = create_model(n_hidden=2, nh_neurons=100, activ=\"relu\", lr=0.0009, batch_norm = False)\r\n",
        "\r\n",
        "# Define the above described clr callback to achieve an optimal learning rate\r\n",
        "step_size = int((5*len(X_train)/50)) # Number of iterations * 5 / batch size\r\n",
        "clr = CyclicLR(base_lr=0.0001, max_lr=0.001, step_size=step_size, mode='triangular')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRUhGJy7xM7X",
        "outputId": "e963dbf6-2914-4486-cc9d-95bb4868ba75"
      },
      "source": [
        "# Train the model with Cyclical LR\r\n",
        "history = model.fit(X_train, y_train, batch_size=50, epochs=40, verbose=1,\r\n",
        "                    validation_data=(X_valid, y_valid), \r\n",
        "                    callbacks=[early_stopping_cb, tensorboard_cb, clr])\r\n",
        "\r\n",
        "# Evaluate the test accuracy of the model\r\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "   3/1080 [..............................] - ETA: 1:35 - loss: 2.3156 - accuracy: 0.1011WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0294s). Check your callbacks.\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 0.6141 - accuracy: 0.8260 - val_loss: 0.1450 - val_accuracy: 0.9563\n",
            "Epoch 2/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.1205 - accuracy: 0.9633 - val_loss: 0.1418 - val_accuracy: 0.9560\n",
            "Epoch 3/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0853 - accuracy: 0.9741 - val_loss: 0.1127 - val_accuracy: 0.9643\n",
            "Epoch 4/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0777 - accuracy: 0.9755 - val_loss: 0.1351 - val_accuracy: 0.9605\n",
            "Epoch 5/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0721 - accuracy: 0.9781 - val_loss: 0.0987 - val_accuracy: 0.9740\n",
            "Epoch 6/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0946 - val_accuracy: 0.9738\n",
            "Epoch 7/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0372 - accuracy: 0.9886 - val_loss: 0.0789 - val_accuracy: 0.9778\n",
            "Epoch 8/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 0.0187 - accuracy: 0.9941 - val_loss: 0.0694 - val_accuracy: 0.9842\n",
            "Epoch 9/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 0.0097 - accuracy: 0.9967 - val_loss: 0.0704 - val_accuracy: 0.9840\n",
            "Epoch 10/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0737 - val_accuracy: 0.9852\n",
            "Epoch 11/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 7.3314e-04 - accuracy: 0.9998 - val_loss: 0.0865 - val_accuracy: 0.9850\n",
            "Epoch 12/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 0.0950 - val_accuracy: 0.9793\n",
            "Epoch 13/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.0768 - val_accuracy: 0.9820\n",
            "Epoch 14/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0209 - accuracy: 0.9943 - val_loss: 0.0808 - val_accuracy: 0.9793\n",
            "Epoch 15/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0323 - accuracy: 0.9915 - val_loss: 0.0959 - val_accuracy: 0.9745\n",
            "Epoch 16/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.0330 - accuracy: 0.9903 - val_loss: 0.0777 - val_accuracy: 0.9817\n",
            "Epoch 17/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 0.0741 - val_accuracy: 0.9827\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0833 - accuracy: 0.9843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08330404758453369, 0.9843000173568726]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YSH2WvDrt7Q"
      },
      "source": [
        "# Here we apply a change to the best_hyp function, such that from now on the cyclical LR callback is used\r\n",
        "\r\n",
        "# Create a function to perform grid search\r\n",
        "def best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[0,2], nh_neurons_range=[30,50,70,100], \r\n",
        "             activations=[\"relu\",\"elu\"], lr_range=[0.001,0.0005], batch_norm_range=[True,False]):\r\n",
        "  # Create a variable to store the best score seen so far\r\n",
        "  best_score = 0\r\n",
        "  # To prevent overfitting, we introduce early stopping, such that the training is stopped as soon as no improovement has been monitored over 7 periods according to the validation accuracy\r\n",
        "  early_stopping_cb = EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)\r\n",
        "  # Set up a loop to go through all possible parameter combinations\r\n",
        "  for n_hidden in range(n_hidden_range[0],(n_hidden_range[1]+1)):\r\n",
        "    for nh_neurons in nh_neurons_range:\r\n",
        "      for activ in activations:\r\n",
        "        for lr in lr_range:\r\n",
        "          for batch_norm in batch_norm_range:\r\n",
        "            model = create_model(n_hidden, nh_neurons, activ, lr, batch_norm)\r\n",
        "            # Train the model\r\n",
        "            model.fit(X_train, y_train, batch_size=50, epochs=40, verbose=0,\r\n",
        "                      validation_data=(X_valid, y_valid), \r\n",
        "                      callbacks=[early_stopping_cb,clr])\r\n",
        "            loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "            print(\"The parameter combination\", [n_hidden, nh_neurons, activ, lr, batch_norm], \"leads to a validation accuracy of\", model.evaluate(X_valid, y_valid, verbose=0)[1])\r\n",
        "            if accuracy > best_score:\r\n",
        "              best_score = accuracy\r\n",
        "              best_model = model\r\n",
        "              best_params = [n_hidden, nh_neurons, activ, lr, batch_norm]\r\n",
        "              print(\"The best test accuracy so far is:\", best_score)\r\n",
        "\r\n",
        "  return best_model, best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJQZz1Rbb7P",
        "outputId": "a253b4af-9ca7-4560-9791-672ac003e51d"
      },
      "source": [
        "# Apply the best_hyp function to decide whether batch normalization should be applied or not\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[2,2], \r\n",
        "                         nh_neurons_range=[100], activations=[\"relu\"], lr_range=[0.0009],\r\n",
        "                         batch_norm_range=[False, True])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameter combination [2, 100, 'relu', 0.0009, False] leads to a validation accuracy of 0.9836666584014893\n",
            "The best test accuracy so far is: 0.9850999712944031\n",
            "The parameter combination [2, 100, 'relu', 0.0009, True] leads to a validation accuracy of 0.9828333258628845\n",
            "The best parameters are: [2, 100, 'relu', 0.0009, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nerwcxXc2b3"
      },
      "source": [
        "# Here we apply a minor change to the best_hyp function, to prevent that for each parameter combination an output is produced\r\n",
        "\r\n",
        "# Create a function to perform grid search\r\n",
        "def best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[0,2], nh_neurons_range=[30,50,70,100], \r\n",
        "             activations=[\"relu\",\"elu\"], lr_range=[0.001,0.0005], batch_norm_range=[True,False]):\r\n",
        "  # Create a variable to store the best score seen so far\r\n",
        "  best_score = 0\r\n",
        "  # To prevent overfitting, we introduce early stopping, such that the training is stopped as soon as no improovement has been monitored over 7 periods according to the validation accuracy\r\n",
        "  early_stopping_cb = EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)\r\n",
        "  # Set up a loop to go through all possible parameter combinations\r\n",
        "  for n_hidden in range(n_hidden_range[0],(n_hidden_range[1]+1)):\r\n",
        "    for nh_neurons in nh_neurons_range:\r\n",
        "      for activ in activations:\r\n",
        "        for lr in lr_range:\r\n",
        "          for batch_norm in batch_norm_range:\r\n",
        "            model = create_model(n_hidden, nh_neurons, activ, lr, batch_norm)\r\n",
        "            # Train the model\r\n",
        "            model.fit(X_train, y_train, batch_size=50, epochs=40, verbose=0,\r\n",
        "                      validation_data=(X_valid, y_valid), \r\n",
        "                      callbacks=[early_stopping_cb,clr])\r\n",
        "            loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "            if accuracy > best_score:\r\n",
        "              best_score = accuracy\r\n",
        "              best_model = model\r\n",
        "              best_params = [n_hidden, nh_neurons, activ, lr, batch_norm]\r\n",
        "              print(\"The best test accuracy so far is:\", best_score)\r\n",
        "\r\n",
        "  return best_model, best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UilcccffdAg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26840a5e-14b1-431b-a738-b514500e76c7"
      },
      "source": [
        "# Apply grid search to find the best hyperparameters\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[1,3], \r\n",
        "                         nh_neurons_range=[70,80,90,100], activations=[\"relu\",\"elu\",\"selu\"], lr_range=[0.0009],\r\n",
        "                         batch_norm_range=[False,True])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best test accuracy so far is: 0.9815999865531921\n",
            "The best test accuracy so far is: 0.9848999977111816\n",
            "The best test accuracy so far is: 0.9850999712944031\n",
            "The best test accuracy so far is: 0.9855999946594238\n",
            "The best test accuracy so far is: 0.986299991607666\n",
            "The best test accuracy so far is: 0.9865000247955322\n",
            "The best parameters are: [2, 80, 'selu', 0.0009, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjqecXz7o-2m",
        "outputId": "1e580e92-4390-4bf3-d870-b4ad1aecaeef"
      },
      "source": [
        "# Apply grid search to find the best hyperparameters\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[1,3], \r\n",
        "                         nh_neurons_range=[70,80,90,100], activations=[\"relu\",\"elu\",\"selu\"], lr_range=[0.0009],\r\n",
        "                         batch_norm_range=[False,True])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best test accuracy so far is: 0.9817000031471252\n",
            "The best test accuracy so far is: 0.9822999835014343\n",
            "The best test accuracy so far is: 0.9837999939918518\n",
            "The best test accuracy so far is: 0.9850000143051147\n",
            "The best test accuracy so far is: 0.98580002784729\n",
            "The best test accuracy so far is: 0.9865999817848206\n",
            "The best test accuracy so far is: 0.9868999719619751\n",
            "The best parameters are: [2, 70, 'elu', 0.0009, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_ZrMi1QH1cj",
        "outputId": "aadd3918-b259-483d-d0c5-3e557d63d2b4"
      },
      "source": [
        "# Apply grid search to find the best hyperparameters\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[2,2], \r\n",
        "                         nh_neurons_range=[65,70,75,80,85], activations=[\"elu\",\"selu\"], lr_range=[0.0009],\r\n",
        "                         batch_norm_range=[True])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best test accuracy so far is: 0.9830999970436096\n",
            "The best test accuracy so far is: 0.9860000014305115\n",
            "The best test accuracy so far is: 0.9876000285148621\n",
            "The best parameters are: [2, 85, 'elu', 0.0009, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIHS1r6Zewxt",
        "outputId": "f5c7fba7-1e0c-480e-b552-82f9378e8629"
      },
      "source": [
        "# Apply grid search to find the best hyperparameters\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[2,2], \r\n",
        "                         nh_neurons_range=[65,70,75,80,85], activations=[\"elu\",\"selu\"], lr_range=[0.0009],\r\n",
        "                         batch_norm_range=[True])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best test accuracy so far is: 0.9825999736785889\n",
            "The best test accuracy so far is: 0.9850999712944031\n",
            "The best test accuracy so far is: 0.9873999953269958\n",
            "The best parameters are: [2, 70, 'elu', 0.0009, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEo6KSXbiwtV",
        "outputId": "b82590b9-d54c-4a76-e206-1dbcc4f78a25"
      },
      "source": [
        "# Apply grid search to find the best hyperparameters\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[2,2], \r\n",
        "                         nh_neurons_range=[67,68,69,70,71,72,73], activations=[\"elu\"], lr_range=[0.0009],\r\n",
        "                         batch_norm_range=[True])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best test accuracy so far is: 0.9828000068664551\n",
            "The best test accuracy so far is: 0.9853000044822693\n",
            "The best parameters are: [2, 69, 'elu', 0.0009, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs_TUEVEbIYC",
        "outputId": "95b684d9-60dc-4457-d810-a31bbe9f28e7"
      },
      "source": [
        "# Apply grid search to find the best hyperparameters\r\n",
        "model, params = best_hyp(X_train, y_train, X_valid, y_valid, X_test, y_test, n_hidden_range=[2,2], \r\n",
        "                         nh_neurons_range=[82,83,84,85,86,87,88], activations=[\"elu\"], lr_range=[0.0009],\r\n",
        "                         batch_norm_range=[True])\r\n",
        "print(\"The best parameters are:\", params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best test accuracy so far is: 0.9825999736785889\n",
            "The best test accuracy so far is: 0.9861000180244446\n",
            "The best test accuracy so far is: 0.9866999983787537\n",
            "The best parameters are: [2, 86, 'elu', 0.0009, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwSd51hiddvV"
      },
      "source": [
        "# Create a function to train a model repeatedly and return the best resulting model\r\n",
        "\r\n",
        "def rep_fit(params, repetitions=10):\r\n",
        "  # Define the callbacks we want to use\r\n",
        "  ## Early stopping\r\n",
        "  early_stopping_cb = EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)\r\n",
        "  ## Cyclical LR\r\n",
        "  step_size = int((5*len(X_train)/50)) # Number of iterations * 5 / batch size\r\n",
        "  clr = CyclicLR(base_lr=0.0001, max_lr=0.001, step_size=step_size, mode='triangular')\r\n",
        "\r\n",
        "  # Create a variable to store the best score seen so far\r\n",
        "  best_score = 0\r\n",
        "\r\n",
        "  # Repeatedly fit the same model\r\n",
        "  for i in range(repetitions):\r\n",
        "    # Create and compile the model\r\n",
        "    model = create_model(params[0], params[1], params[2], params[3], params[4])\r\n",
        "    # Train the model\r\n",
        "    history = model.fit(X_train, y_train, batch_size=50, epochs=40, verbose=0,\r\n",
        "                        validation_data=(X_valid, y_valid), \r\n",
        "                        callbacks=[early_stopping_cb,clr])\r\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "    if accuracy > best_score:\r\n",
        "      best_score = accuracy\r\n",
        "      best_model = model\r\n",
        "      best_history = history\r\n",
        "      print(\"The best test accuracy so far is:\", best_score)\r\n",
        "\r\n",
        "  return best_model, best_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7YbFqwXogSO"
      },
      "source": [
        "params1 = [2, 69, 'elu', 0.0009, True]\r\n",
        "params2 = [2, 86, 'elu', 0.0009, True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR9b_HfpeNz-",
        "outputId": "535fc4f9-e893-4f60-855a-83fa65c4699b"
      },
      "source": [
        "# Repeatedly train the model with the best specifications\r\n",
        "model, history = rep_fit(params=params1, repetitions=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best test accuracy so far is: 0.9857000112533569\n",
            "The best test accuracy so far is: 0.986299991607666\n",
            "The best test accuracy so far is: 0.9864000082015991\n",
            "The best test accuracy so far is: 0.9873999953269958\n",
            "The best test accuracy so far is: 0.9879000186920166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psV2hIJcMsCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f0cc3b-3eb3-4790-86c2-c29479fc3276"
      },
      "source": [
        "# Repeatedly train the model with the best specifications\r\n",
        "model, history = rep_fit(params=params2, repetitions=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best test accuracy so far is: 0.9850999712944031\n",
            "The best test accuracy so far is: 0.9861000180244446\n",
            "The best test accuracy so far is: 0.9865999817848206\n",
            "The best test accuracy so far is: 0.9869999885559082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F0CxYl9NGqw"
      },
      "source": [
        "# Start TensorBoard\r\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "Wd6Z4YbQBHbJ",
        "outputId": "3824db38-675c-4cd8-8487-5de1e81ca223"
      },
      "source": [
        "# Show the learning curves\n",
        "## Note: It is assumed that the training history is stored in the variable \"history\"\n",
        "pd.DataFrame(history.history).plot()\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fOc2AlNBHbI",
        "outputId": "8aadc781-05c3-448b-a321-97c008177b4f"
      },
      "source": [
        "# show the structure of your model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_40 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_234 (Dense)            (None, 552)               433320    \n",
            "_________________________________________________________________\n",
            "batch_normalization_152 (Bat (None, 552)               2208      \n",
            "_________________________________________________________________\n",
            "dense_235 (Dense)            (None, 552)               305256    \n",
            "_________________________________________________________________\n",
            "batch_normalization_153 (Bat (None, 552)               2208      \n",
            "_________________________________________________________________\n",
            "dense_236 (Dense)            (None, 276)               152628    \n",
            "_________________________________________________________________\n",
            "batch_normalization_154 (Bat (None, 276)               1104      \n",
            "_________________________________________________________________\n",
            "dense_237 (Dense)            (None, 138)               38226     \n",
            "_________________________________________________________________\n",
            "batch_normalization_155 (Bat (None, 138)               552       \n",
            "_________________________________________________________________\n",
            "dense_238 (Dense)            (None, 69)                9591      \n",
            "_________________________________________________________________\n",
            "dense_239 (Dense)            (None, 10)                700       \n",
            "=================================================================\n",
            "Total params: 945,793\n",
            "Trainable params: 942,757\n",
            "Non-trainable params: 3,036\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2TlREIjBHbJ"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_SnVFnVBHbJ"
      },
      "source": [
        "model.save('mnist.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "akmqpH7w4w8Q",
        "outputId": "409262dc-096b-4dc5-c8ab-1c7d90e667a0"
      },
      "source": [
        "# Download the model directly\r\n",
        "from google.colab import files\r\n",
        "files.download('mnist.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_49f002f3-1d1f-4abe-96a3-df970371ddca\", \"mnist.h5\", 11417992)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wnacqxVBHbJ"
      },
      "source": [
        "## Load the model and evaluate it on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK_5oagDBHbK",
        "outputId": "59fa2579-65b6-4241-aa57-1e6ad642e2c6"
      },
      "source": [
        "model = keras.models.load_model(\"mnist.h5\")\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0470 - accuracy: 0.9879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.046961717307567596, 0.9879000186920166]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh7V9fmCpUjg",
        "outputId": "23636ca9-9d45-4374-c520-ecce73fb6215"
      },
      "source": [
        "# OWN: Model Evaluation\r\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0470 - accuracy: 0.9879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.046961717307567596, 0.9879000186920166]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxkrYQVVBHbK"
      },
      "source": [
        "## Short report\n",
        "\n",
        "Please write briefly how you build and train the model. Please include the decisions you made, such as why you use x number layers, and the difficulties you met."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJFy3pvABHbK"
      },
      "source": [
        "## Introduction \r\n",
        "\r\n",
        "In the following part we will give insight into the creation of the model, the assumptions we based our analysis on as well as the difficulties we faced while compiling the algorithm. \r\n",
        "\r\n",
        "In order to compile the algorithm, we first had to choose a random model to build our specifications on.\r\n",
        "\r\n",
        "As such, we decided to follow an approach in which we use four hidden layers with an initial 500 neurons in the first layer, 200 in the second, 100 in the third and 50 in the fourth layer. We decided to choose this setting through random assignment of the number of hidden layers as well as neurons in the initial layer (we defined a range of 300 to 600 neurons, in steps of 100, as well as a range of 3 to 7 layers to choose randomly from and obtained the values of 500 and 4, respectively). Then, we deemed it necessary to follow the approach shown in the lecture and design a \"triangle-like\" type of network, which gradually declines in neurons per layer until it reaches its ouput layer. A possible intuition behind this might be that we can, similarly to a pooling layer, somehow decrease the density and, thus, decrease the potential to overfit throughout the layers.\r\n",
        "\r\n",
        "Moreover, we defined the initial activation functions to be relu, which are initialised with the he_normal initializer. This decision was made on the basis of saturation as well as vanishing gradient issues, which relu is known to account for to some extent. Further, since we want a sparely-activated function, we can also profit of relu's one-sided activation characteristic. We then used the he_normal initializer to account for an improved weight initialisation, which was recommended by Haozhe in the Practical Session. \r\n",
        "\r\n",
        "Also, we defined an initial optimizer, which is given as Adam optimizer. This choice was also done based on recommendations given in the Practical Session. Especially, we oriented ourselves on papers (Diederik and Ba (2014), Smith (2017)) which showed the superiority of the Adam operator in optimization processes with regard to Momentum and adaptive learning rate optimizations. \r\n",
        "\r\n",
        "Lastly, we use a softmax activation function as well as a categorical cross-entropy loss as best practice.\r\n",
        "\r\n",
        "In order to monitor the development of the performance of the model during training, we decide to take 6000 randomly chosen observations (i.e. 10%) of the full training data as validation data and evaluate the validation loss and validation accuracy after each epoch. We then decide to consider the validation accuracy as the best proxy for the test accuracy, which is the objective we want to maximize.\r\n",
        "\r\n",
        "## Optimisation\r\n",
        "\r\n",
        "Before starting with the actual model optimization, we had to evaluate whether a \"triangle-shaped\" neural net, implying constantly decreasing number of neurons per layer, is superior to a model which has a constant and stepwise decline of neurons per layer. Doing so would require to perform parameter updating for different model architectures. However, an issue that came to our mind was that, potentially, each architecture is differently impacted or differently impacts the other hyperparameters. As such, a different network architecture might be more prone to overfitting or could potentially better equip a certain activation function. Thus, we decided that we should solely focus on the most used network architecture for these settings, which is based on a constantly declining number of neurons (triangle shape), as otherwise the existing considerations for hyperparameter combinations might not hold any longer. \r\n",
        "\r\n",
        "Starting from the setting defined above, we subsequently perform hyperparameter tuning. An important question arises with regard to the order in which each hyperparameter should be tuned. This is not an intuitive point. However, when sourcing for this question online, the general opinion suggests that first the most fundamental parts of the network, such as batch sizes, epochs, hidden layers as well as number of neurons per hidden layer, should be updated before focusing on optimizing parameters and activation functions. Lastly, one should focus on learning rates and dropout regularizers. This is in part because, as argued, the learning rate is an essential parameter of the model which is likely to influence other parameters. Also, regularizer terms should be obtained at the end to account for the effective risk of overfitting. However, we decided to combat overfitting by introducing an early stopping callback, which automatically stops the training process as soon as the accuracy on the validation set did no longer increase over 7 epochs (this number has been chosen by intuitive considerations), and then chooses the model from the last 7 epochs that exhibited the largest validation accuracy. With this strategy we intend to reduce the number of hyperparameters we need to tune, since the necessity of dropout layers and kernel regularization can be avoided and the number of epochs can simply be set to an arbitrarily high number (e.g. 40). Furthermore, to enable convenient deep insights to the model we train, we additionally introduce the tensorboard callback.\r\n",
        "\r\n",
        "As such, we follow this order when tuning the hyperparameters: \r\n",
        "\r\n",
        "- (Number of Epochs)\r\n",
        "- Batch Size\r\n",
        "- Number of Hidden Layers\r\n",
        "- Number of Neurons per Hidden Layer\r\n",
        "- Activation function\r\n",
        "- Learning Rate (also experimented with CyclicLR)\r\n",
        "- Batch-normalization\r\n",
        "- (Drop out rate)\r\n",
        "- (Kernel regularization)\r\n",
        "\r\n",
        "We then chose the hyperparameters as stated above (note that the hyperparameters listed in brackets are not optimized/set in the following, as the early stopping callback already accounts for the overfitting issue). Certainly, we are aware of the fact that one could choose different orders. However, this was the most commonly used order for training artificial neural nets we encountered during our research process.\r\n",
        "\r\n",
        "During the process, we encountered some difficulties. Realizing that performing a full grid search would take a considerable amount of time, we decide to apply the following strategy. First we search for an optimal batch size using the initial model defined above. For computational reasons, we then decided to only include the batch size that appeared to lead to the highest validation set accuracy in the initial model. Then we continue by randomly trying some intuitively/reasonably chosen values for the subsequent hyperparameter in the list stated above. The aim behind this step is to disclose a region of values for each hyperparameter, for which the respective hyperparamter appears to lead to a good performing model. This is done for each hyperparameter separeately and in the order stated above. When continuing to the next hyperparameter, the previously investigated hyperparameters are fixed to a value in the \"optimal region\" we obtained. In the end, we perform grid search using values from each hpyerparameter's revealed \"optimal region\" (except for the batch size, which we keep fixed). Furthermore, as soon as the best performing hyperparameter constellation is found, we further investigate a grid covering its close neighbourhood to hopefully find an even better performing model.\r\n",
        "\r\n",
        "As such, we decided to start training our initial model with 8 different batch sizes, ranging from 30 to 100 by steps of 10. Doing so reveals that a batch size of 50 appears to be the best choice. Thus, we coninue to search optimal regions for the remaining hyperparameters as listed above. To do so we define the function **create_model** to create and compile a model depending on several parameters one can choose. The baseline model is similar to the initial model described above (in the sense that the same optimizer and early stopping callback are used). However, since we want the layer structure to be selectable, we use the follwong setup as a basis: a flatten layer in the beginning, a selectable number of low level hidden layers thereafter and three high level hidden layers in the end of the hidden section. The very last layer is then the output layer with 10 neurons and a softmax activation function (as is common for 10-class classification settings). The parameters one can specify in this function are the number of low level hidden layers (i.e. the argument **n_hidden**), the number of neurons in the last high level hidden layer (i.e. the argument **nh_neurons**), the activation function for the hidden layers (i.e. the argument **activ**), the learning rate of the Adam optimizer (i.e. the argument **lr**) and whether each hidden layer should be followed by a batch normalization layer or not (i.e. the boolean argument **batch_norm**). To ensure the triangle shape among the hidden layers as described above, we implemented the function such that one only has to specify the number of neurons in the deepest/last hidden layer (i.e. **nh_neurons**) - the preceding hidden layers then take an increasing number of neurons depending on **nh_neurons** up to a maximum of 560 neurons. We decided to set this ceiling value to prevent creating models with extremely many neurons in the low level hidden layers which are probably rather prone to overfitting. Thereafter, we defined a further function (i.e. the function **best_hyp**) which performs grid search among a specified grid consisting of the parameters one can choose in the **create_model** function. The **best_hyp** function then tries every possible combination in the specified grid (trained with a batch size of 50), chooses the model with the highest validation accuracy for each combination and calculates the test accuracy thereafter. After models for all combinations have been trained, the function returns the model and the paramter combination that led to the highest test accuracy. \r\n",
        "\r\n",
        "We then use this function to sequentially find an \"optimal region\" for all remaining hyperparameters. Thus, we start our search for the optimal number of hidden layers and use the following strategy: first, we set all hyperparameters listed below the number of hidden layers in the above list to a fixed value, such that the resulting model best mimics the initial model as described above (namely, we set the parameters as follows: nh_neurons = 50, activ = relu, lr = 0.001, batch_norm = False). Then, we train this model using 6 different numbers of low level hidden layers ranging from 0 to 5, which implies a range for the total number of hidden layers of 3 to 8. Doing so reveals that best number of low level hidden layers appears to be 2, which implies that 5 hidden layers in total seem to be optimal. However, since the validation accuracies of the models with 1 or 3 low level hidden layers also seem to be comparatively quite high, we define the set {1,2,3} to be the \"optimal region\" for this hyperparameter.\r\n",
        "\r\n",
        "Thereafter, we fix the argument **n_hidden_range** to [2,2] and repeat the same strategy to find an optimal region for the next hyperparameter in the list - namely the number of neurons in the deepest/last hidden layer (which also influences the number of neurons in all of the other hidden layers). Here we try a grid from 30 to 100 neurons by steps of 10 (i.e. 8 different values), while all other parameters are kept fix as before. We observe that choosing 100 neurons leads to the best performance according to validation and test accuracy of the model. However, models with 70, 80 or 90 neurons in the last hidden layer appear to also perform reasonably well. Hence, we define the set {70,80,90,100} to be the \"optimal region\" for this hyperparameter.\r\n",
        "\r\n",
        "We then fix the argument **nh_neurons_range** to 100 and repeat the same strategy to find an optimal set of acivation functions. Since we exclusively want to use activation functions which account for saturation and the vanishing/exploding gradient problem and also are sparely-activated, we decide to restrict the set of activation functions to use in the trial to {\"relu\",\"elu\",\"selu\"}. Considering a theoretical perspective we were uncertain whether to use a selu, relu or elu activation function, as all appear to have similar characteristics. According to Géron (2017), selu generally outperforms relu as the former is able to self-normalise and tend to preserve an output value with mean 0 and SD 1 during training, solving the vanishing gradient problem better than the relu. However, they also claim that this is rather apperent in especially deep neural networks. But having set the comparison, we figured out that using a selu activation function with a \"lecun_normal\" initializer in our model performs similarly well as the relu or elu activation function with \"he_normal\" initialization (note that the **create_model** function is implemented in such a way that the kernel initialization is automatically set to \"lecun_normal\" when a selu activation function is used and set to \"he_normal\" otherwise). Hence, we define the set {\"relu\",\"elu\",\"selu\"} to be an \"optimal set\" of activation functions to try among in the comprehensive grid search later on. However, since in our trial the relu activation function appears to lead to the highest test accuracy, we fix the argument **activations** to \"relu\" and continue with searching an optimal learning rate.\r\n",
        "\r\n",
        "With respect to the learing rate we try a grid of values from 0.0001 to 0.001 by steps of 0.0001 (i.e. 10 different values). Here we observe somewhat ambiguous results, since values from both edges of the grid appear to perform similarly well in predicting the validation set. Since the choice of an approriate learing rate is crucial to help the model to converge more quickly, we apply a more sophisticated approach in the following. As we know, we initialised the optimizer to be Adam based on the notion that it combines two impotant considerations: Adaptive Learning rates and Momentum. As is known, Adam makes use of two sets of Moments; the first, mean-based moment as well as the second, decentralised variance moment. It also defines a certain \"decay rate\" which defines in which interval, or length, the moments should be adjusted. This rate is indicated by $\\beta$ values, empirically found to be optimal at 0.9 and 0.99, respectively (which is the default setting). However, we decide to further tune this hyperparameter by following the approach proposed by Smith (2017). He argues that using a cyclical learning rate can additionally increase model performance as it changes parameters within a pre-defined interval according to some cycles. The idea of introducing a cyclical learning rate is to use a learning rate parameter which changes in magnitude for each cycle to gradually converge to an optimal rate such that training time and model accuracy can be improved. CLR means that the learning rate transgresses within a pre-defined interval. As such, it should address the problems of saddle points which lead to non globally optimal moments. In general, if the learning rate is at the upper bound and has a high value, the model is able to diverge quickly from saddle points, which is also given by the momentum strategy. However, this is complemented now by a lower bound, allowing us to more quickly approach a global minimum. As such, we decided to additionally constrain the optimizer function in combination with the Adam optimizer. We are then expected not only to have an adaptive learning rate focusing on two distinct moments, but also to define a gradual direction of the adaptive learning rate including decay parameters and a given range. That is, we put an additional, iterative constraint on our momentum strategy with Adam and its beta values. To set up the cyclical learning rate callback appropriately, we are required to find some upper and lower bound as well as a cycle length, indicating the length of each max-min circuit. To do so, we follow the argumentation of Smith (2017): \r\n",
        "\r\n",
        "1. The cycle length (or step size) should be between 2-10 times the number of iterations divided by the batch size. Hence, we choose the step size to be $\\frac{5*N}{10}$ (where N stands for the number of observations in the training data).\r\n",
        "2. The min and max values for the learning rate are given through an iterative training process similar to defining the initial learning rate in different settings. We just ran the model for different learning rates between 0.0001 and 0.001 and observed that the performance of the models did not vary too much. Hence, we choose 0.0001 as the lower bound and 0.001 as the upper bound.\r\n",
        "\r\n",
        "Another advantage when working with a cyclical learning rate is that the selection of the initial learning rate becomes a minor concern, as the learning is anyway changing during the training process. Hence, we simply set the initial learing rate to be 0.0009, as this value appeared to lead to the best performing model in the according grid search described above. We observe that using a cyclical learning rate indeed increases the prediction accuracy of the model, while also causing the model to converge rather quickly. Hence, we decide to apply the cyclical learing rate strategy with an initial learning rate fixed to 0.0009 in all of the subsequent models.\r\n",
        "\r\n",
        "Lastly, we wanted to assess whether batch normalization leads to a substantial improvement of the mode performance. Running the model (with all previously analyzed hyperparameters fixed as described above) once with batch normalization and once without reveals that both approaches lead to similar results. Hence, we decide to keep both options for the **batch_norm** argument - True and Flase - in the grid while performing the comprehensive grid search.\r\n",
        "\r\n",
        "We then were set to perform the final grid search procedures using values from each hpyerparameter's revealed \"optimal region\". To gain robustness we perform the same grid search twice, to check whether the resulting best models are similar in both runs. Doing so reveals that the best performing model has the following hyperparameter constellation: 5 hidden layers, 70 or 80 neurons in the last hidden layer, selu or elu activation function, cyclical learing rate with 0.0009 as the initial value and batch normalization activated.\r\n",
        "\r\n",
        "To proceed, we decide to fix the number of hidden layers to be 5 and the batch normalization to remain avtivated. Furthermore, we refine the grid for the number of neurons in the deepest/last hidden layer such that it is {65,70,75,80,85} and reduce the set of activation functions to {elu,selu}. We then left the initial learning rate unchanged and repeated the grid search (to gain robustness, we again performed the grid search twice for this setting). Doing so reveals that the best performing model has the following hyperparameter constellation: 5 hidden layers, once 70 and once 85 neurons in the last hidden layer, elu activation function, cyclical learing rate with 0.0009 as the initial value and batch normalization activated.\r\n",
        "\r\n",
        "We then turned to the last grid search section. Since we observed that the elu activation function (with he_normal weight initialization) appears to lead to the best performing models for two different number of neurons in the last hidden layer (i.e. 70 and 85), we decide to fix the activation functions to be elu and search for good performing models in the close neighbourhood of both optimal number of neurons for the ultimate hidden layer. Hence, we perform a grid search using the following two grids for the number of neurons in the last hidden layer: {67,68,69,70,71,72,73} and {82,83,84,85,86,87,88}. The remaining hyperparamters are again left unchanged. Doing so reveals that the following two hyperparameter constellations appear to be optimal:\r\n",
        "\r\n",
        "1. 5 hidden layers, 69 neurons in the last hidden layer, elu activation function, cyclical learing rate with 0.0009 as the initial value and batch normalization activated\r\n",
        "\r\n",
        "2. 5 hidden layers, 86 neurons in the last hidden layer, elu activation function, cyclical learing rate with 0.0009 as the initial value and batch normalization activated\r\n",
        "\r\n",
        "As a last step we then trained models with these two specifications multiple times, and selected the model that appeared to lead to the highest prediction accuracy on the test set as our final model.\r\n",
        "\r\n",
        "## Assessment\r\n",
        "\r\n",
        "Having followed state-of-the-art methods defined on our own setting, we are able to construct an iterative tuning process in which we re-evaluated the most prominent parameters of a neural network. All things considered, the iterative process was able to suit the requirements given by the task and even to exceed them. With an overall test set accuracy of 0.9879, we were able to outperform the requested threshold of 0.97 and obtain a sufficient model. Especially, we were able to understand that a cyclic learning rate, batch normalization as well as early stopping features can substantially improve the accuracy of the network even after the main parameters are optimised. As such, the cyclic learning rate appears to well account for the issue of saddle points during the optimization pocess. Further, the early stopping callback also is able to account well for the case of an overfitting network.\r\n",
        "\r\n",
        "In order to be able to further improve the model, one would require to use different methods, such as convolutional filters and pooling layers."
      ]
    }
  ]
}